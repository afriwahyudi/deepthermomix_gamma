{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' \n",
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.datasplit_module as dsm\n",
    "import modules.trainer_module as tm\n",
    "import modules.dtmpnn as gm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Setup & Seeding ---\n",
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Hard-code final HPO Params ---\n",
    "final_params = {\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-07,\n",
    "    'gd_weight': 1,\n",
    "    'graph_hidden_dim': 16,\n",
    "    'latent_dim': 16,\n",
    "    'context_dim': 16,\n",
    "    'graph_layers': 2,\n",
    "    'constraint_type': 'soft',\n",
    "    'include_gd': False,\n",
    "}\n",
    "print(\"\\n--- final Hyperparameters ---\")\n",
    "for k, v in final_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load Full Dataset ---\n",
    "print(\"\\n--- Loading Full Dataset ---\")\n",
    "from modules.data_pipeline import DataPipeline\n",
    "pipeline = DataPipeline(components_csv='datasets/components.csv')\n",
    "canonical_data, graph_list = pipeline.run_pipeline(raw_csv='datasets/dataset.csv')\n",
    "pipeline.save_canonical_df(canonical_data, 'datasets/canonical_data.csv')\n",
    "\n",
    "train, val, test = \\\n",
    "    dsm.system_disjoint_split(graph_list[:1000], random_state=seed, stratify_by_components=True)\n",
    "full_cv_dataset = train + val\n",
    "\n",
    "final_test_loader = DataLoader(\n",
    "    dataset=test,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    follow_batch=['component_batch']\n",
    ")\n",
    "print(f\"Loaded {len(full_cv_dataset)} total points for K-Fold CV.\")\n",
    "print(f\"Loaded {len(test)} total 'unseen' test data points.\")\n",
    "\n",
    "try:\n",
    "    groups = [g.system_id for g in full_cv_dataset]\n",
    "except AttributeError:\n",
    "    print(\"Warning: '.system_id' not found, trying '.system'.\")\n",
    "    try:\n",
    "        groups = [g.system for g in full_cv_dataset] \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Could not get group IDs for splitting. {e}\")\n",
    "        groups = np.arange(len(full_cv_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. The K-Fold CV  ---\n",
    "print(\"\\n--- STARTING K-FOLD CROSS-VALIDATION ---\")\n",
    "K_FOLDS = 5\n",
    "gkf = GroupKFold(n_splits=K_FOLDS)\n",
    "\n",
    "fold_results = {\n",
    "    'train_rmse': [],\n",
    "    'val_rmse': [],\n",
    "    'history': []\n",
    "}\n",
    "\n",
    "def create_fresh_model():\n",
    "    \"\"\"Helper function to init a new model from scratch.\"\"\"\n",
    "    return gm.DTMPNN(\n",
    "        node_dim=train[0].x.size(1),\n",
    "        edge_dim=train[0].edge_attr.size(1),\n",
    "        graph_hidden_dim=final_params['graph_hidden_dim'],\n",
    "        latent_dim=final_params['latent_dim'],\n",
    "        context_dim=final_params['context_dim'],\n",
    "        graph_layers=final_params['graph_layers'],\n",
    "        constraint_type=final_params['constraint_type']\n",
    "    ).to(device)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(full_cv_dataset, groups=groups)):\n",
    "    print(f\"\\n--- Running Fold {fold+1}/{K_FOLDS} ---\")\n",
    "    \n",
    "    # 1. Get data for this fold\n",
    "    train_fold_data = [full_cv_dataset[i] for i in train_idx]\n",
    "    val_fold_data = [full_cv_dataset[i] for i in val_idx]\n",
    "    \n",
    "    # 2. Create loaders\n",
    "    train_loader = DataLoader(train_fold_data, batch_size=512, shuffle=True, follow_batch=['component_batch'])\n",
    "    val_loader = DataLoader(val_fold_data, batch_size=512, shuffle=False, follow_batch=['component_batch'])\n",
    "    \n",
    "    # 3. Build a BRAND NEW model\n",
    "    model = create_fresh_model()\n",
    "    \n",
    "    trainer = tm.DTMPNNTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=None,\n",
    "        include_gd=final_params['include_gd'],\n",
    "        device=device,\n",
    "        lr=final_params['lr'],\n",
    "        weight_decay=final_params['weight_decay'],\n",
    "        gd_weight=final_params['gd_weight'],\n",
    "    )\n",
    "    \n",
    "    # 4. Train to convergence\n",
    "    c_type = final_params['constraint_type']\n",
    "    gd_inc = final_params['include_gd']\n",
    "\n",
    "    history = trainer.train(\n",
    "        epochs=5, \n",
    "        save_dir=f'notebooks/training_phase/crossval/log/{c_type}_gd_{gd_inc}_cv/fold_{fold+1}',\n",
    "        log_file_path=f'notebooks/training_phase/crossval/log/{c_type}_gd_{gd_inc}_cv/fold_{fold+1}/train.log',\n",
    "        save_best=True, \n",
    "        save_every=None,\n",
    "        patience=25\n",
    "    )\n",
    "    trainer.plot_history(save_path = f'notebooks/training_phase/crossval/log/{c_type}_gd_{gd_inc}_cv/fold_{fold+1}/train.png')\n",
    "    \n",
    "    # 5. Get metrics from the BEST epoch\n",
    "    best_epoch = trainer.best_epoch\n",
    "    val_rmse_at_best = history['val_rmse'][best_epoch-1]\n",
    "    train_rmse_at_best = history['train_rmse'][best_epoch-1]\n",
    "    \n",
    "    print(f\"  Fold {fold+1} Best Epoch: {best_epoch}\")\n",
    "    print(f\"  Fold {fold+1} Train RMSE: {train_rmse_at_best:.6f}\")\n",
    "    print(f\"  Fold {fold+1} Val RMSE:   {val_rmse_at_best:.6f}\")\n",
    "    \n",
    "    # 6. Store results\n",
    "    fold_results['train_rmse'].append(train_rmse_at_best)\n",
    "    fold_results['val_rmse'].append(val_rmse_at_best)\n",
    "    fold_results['history'].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Aggregate CV Results (for Table 1) ---\n",
    "train_mu = np.mean(fold_results['train_rmse'])\n",
    "train_sig = np.std(fold_results['train_rmse'])\n",
    "val_mu = np.mean(fold_results['val_rmse'])\n",
    "val_sig = np.std(fold_results['val_rmse'])\n",
    "\n",
    "print(f\"\\n\\n--- {K_FOLDS}-FOLD CV AGGREGATE RESULTS ---\")\n",
    "print(f\"  CV Train RMSE: {train_mu:.6f} ± {train_sig:.6f}\")\n",
    "print(f\"  CV Val RMSE:   {val_mu:.6f} ± {val_sig:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Train Final Model ---\n",
    "print(\"\\n\\n--- STARTING FINAL MODEL TRAINING ---\")\n",
    "final_train_data = train\n",
    "final_val_data = test\n",
    "print(f\"Final Model: Training on {len(final_train_data)}, Validating on {len(final_val_data)}\")\n",
    "\n",
    "final_train_loader = DataLoader(final_train_data, batch_size=1024, shuffle=True, follow_batch=['component_batch'])\n",
    "final_val_loader = DataLoader(final_val_data, batch_size=1024, shuffle=False, follow_batch=['component_batch'])\n",
    "trained_model = create_fresh_model()\n",
    "trained_trainer = tm.DTMPNNTrainer(\n",
    "        model=model,\n",
    "        train_loader=final_train_loader,\n",
    "        val_loader=final_val_loader,\n",
    "        test_loader=final_test_loader,\n",
    "        include_gd=final_params['include_gd'],\n",
    "        device=device,\n",
    "        lr=final_params['lr'],\n",
    "        weight_decay=final_params['weight_decay'],\n",
    "        gd_weight=final_params['gd_weight'],\n",
    "    )\n",
    "\n",
    "trained_history = trained_trainer.train(\n",
    "    epochs=5,\n",
    "    save_dir=f'notebooks/training_phase/final_model/log/{c_type}_gd_{gd_inc}',\n",
    "    log_file_path=f'notebooks/training_phase/final_model/log/{c_type}_gd_{gd_inc}/train.log',\n",
    "    save_best=True,\n",
    "    save_every=None,\n",
    "    patience=50\n",
    ")\n",
    "save_path = f'notebooks/training_phase/final_model/log/{c_type}_gd_{gd_inc}/final_training_history.png'\n",
    "trained_trainer.plot_history(save_path=save_path)\n",
    "print(f\"\\ntrained model training history saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Final Unbiased Evaluation  ---\n",
    "print(\"\\n--- trained Training Complete ---\")\n",
    "print(f\"Loading best trained model from epoch: {trained_trainer.best_epoch}\")\n",
    "model_path = f'notebooks/training_phase/final_model/log/{c_type}_gd_{gd_inc}/best_model.pt'\n",
    "print(\"Load location:\", model_path)\n",
    "trained_trainer.load_checkpoint(model_path)\n",
    "\n",
    "print(\"\\n--- EVALUATING trained MODEL ON UNSEEN TEST SET ---\")\n",
    "test_loss, test_data_driven, test_gd, test_rmse, test_mae = trained_trainer.validate(final_test_loader)\n",
    "\n",
    "print(\"\\n--- FINAL TEST METRICS (TABLE 1) ---\")\n",
    "print(f\"  Test Loss (Data-driven): {test_loss:.6f}\")\n",
    "print(f\"  Test GD Loss (monitored): {test_gd:.6f}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.6f}\")\n",
    "print(f\"  Test MAE:  {test_mae:.6f}\")\n",
    "print(\"\\n--- RUN COMPLETE ---\")\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vle-dtmpnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
